---
title: Ordinary Least Squares - Linear Regression
date: 2022-04-10 15:00:00 -500
categories: [Supervised Learning]
tags: [ols, linear regression]     # TAG names should always be lowercase
description: I discuss ordinary least squares (OLS) aka linear regression, a common parametric model that optimizes regression coefficients by minimizing the sum of residual squares.  
math: true
---

Suppose we have a dataset of $n$ observations, each consist of $p$ explanatory (independent) variables ${\bf x}_{i}$ (e.g a row vector) and a response/target (dependent) variable $y_i$ for $i = 1,2,\dots n$. We are interested in modeling the relationship between the independent and dependent variables. The simplest relationship we can assume is that the response variable is a linear function of the predictor variables:   

$$
\begin{equation}\label{lr1}
y_{i} = \beta_1\, x_{i,1} + \beta_2\, x_{i,2} + \dots + \beta_p\, x_{i,p} + \epsilon_i,
\end{equation}
$$

where $\beta$'s are unknown parameters/coefficients of our hypothesis and $\epsilon_i$'s are the irreducible scalar error term of each observation that could arise due to measuring error, randomness of the underlying process or the modeling error assumed by the linear relationship \eqref{lr1}. Here, we should think of approximating the *true* relationship $y = f(X) + \epsilon$ by a linear function and $\epsilon$ is the catch-all term that we use to characterize what we miss by this simple model, including modeling errors, measurement errors during the data collection and/or other variables that we did not include in our dataset that can cause a variation in the response $y$. 

We can summarize the information contained in the independent variables into a matrix called the *design* matrix ${\bf X}: n \times p$ and vectorize the corresponding response $\vec{y} = [y_1,y_2,\dots, y_n]^{T}$ and errors $\vec{\epsilon} = [\epsilon_1, \epsilon_2, \dots, \epsilon_n]^{T}$ to write the linear regression model \eqref{lr1} compactly as 

$$
\begin{equation}\label{lrmf}
\vec{y} = {\bf X} \vec{\beta} + \vec{\epsilon},
\end{equation}
$$

by defining the $p \times 1$ coefficient vector $\vec{\beta} = [\beta_1,\beta_2,\dots, \beta_p]^T$. From \eqref{lrmf}, it is clear that the vector $\vec{\beta}$ represents the average increase in the random variable $\vec{y}$ associated with a unit increase in ${\bf X}$. In linear regression, it is also customary to add a constant intercept term $\beta_0$, representing the expected value of $\vec{y}$ when ${\bf X} = 0$. In the formulation above, this can be achieved e.g. by adding a vector with unit components as the first column of ${\bf X}$. I will discuss more on this issue later on.

**Errors vs residuals.** Given the hypothesis in \eqref{lrmf}, our goal is to estimate the optimal coefficients $\hat{\vec{\beta}}$ such that the resulting response predictions $\hat{\vec{y}}$ are as close as the observed values $\vec{y}$. These predictions will not be perfect, and therefore we should quantify this error. The terms used to quantify this error is called the *residuals* which we will denote by a vector $\vec{\mathrm{e}} = [\mathrm{e}_1, \mathrm{e}_2, \dots, \mathrm{e}_n]^T$: 

$$
\begin{align}
\vec{\mathrm{e}} &\equiv \vec{y} - \hat{\vec{y}},\\
&= \vec{y} - {\bf X}\hat{\vec{\beta}}, 
\end{align}
$$

where $\hat{\vec{y}} = {\bf X} \hat{\vec{\beta}}$ is estimated response arise from the estimated linear regression coefficients $\hat{\vec{\beta}}$. An important distinction here is that *residuals* are not the same as the error terms $\vec{\epsilon}$ in \eqref{lrmf}. The latter quantifies the difference between the observed response $\vec{y}$ and the "true" data ${\bf X}\vec{\beta}$ that we assume to hold. $\vec{\epsilon} = \vec{y} - {\bf X}\vec{\beta}$. In other words, they are errors related to the true data generating process ${\bf X}\vec{\beta}$ while the *residuals* are the ones associated with the estimated model ${\bf X}\hat{\vec{\beta}}$. In principle, we can make perfect predictions $\vec{y} = \hat{\vec{y}}$ with zero residuals, but this does not mean that the true errors are zero, and in fact they are still unknown. 

#### Least Squares Criterion and Normal Equations
------

In our model, making a prediction for the response variable corresponds implies making predictions for the coefficients $\beta$. What criterion should we use to determine the optimal coefficients? In ordinary least squares (OLS) regression, we simply use the *sum of squared residuals* as the loss function as the function to be minimized. Then the best fit coefficients $\vec{\beta} = [\beta_1,\beta_2, \dots, \beta_p]^T$ are those that minimizes the loss function: 

$$
\begin{align}
\nonumber \hat{\vec{\beta}} &= \textrm{argmin}_{\vec{\beta}}\,\,L(\vec{\beta}),\\
&= \textrm{argmin}_{\vec{\beta}}\,\, (\vec{y} - {\bf X} \vec{\beta})^{T}(\vec{y} - {\bf X} \vec{\beta}). 
\end{align}
$$

The optimization problem is then solved by setting the first derivative of the loss function w.r.t the coefficients $\vec{\beta}$ zero 

$$
\begin{equation}\label{lfd}
\frac{\partial L(\vec{\beta})}{\partial \vec{\beta}} = - 2 {\bf X}^{T} \left(\vec{y} - {\bf X} \vec{\beta}\right) \quad \Longrightarrow \quad {\bf X}^{T} \left(\vec{y} - {\bf X} \vec{\beta}\right) = 0.
\end{equation}
$$

The expression on the right is called the normal equations which are a set of ($p$ number of) equations in disguise. Notice from \eqref{lfd} that the second derivative of the loss function is postive definite, ensuring that the following solution is a minimum 

$$
\begin{equation}\label{sol}
\hat{\vec{\beta}} = ({\bf X}^{T}\, {\bf X})^{-1}\, {\bf X}^{T}\, \vec{y}. 
\end{equation}
$$

For a unique solution in the form of \eqref{sol} exist, the $p \times p$ matrix ${\bf M} = {\bf X}^T {\bf X}$ must be invertible. Since $M$ is positive definite we know indeed that it has a positive determinant $\textrm{det}({\bf M}) = \textrm{det}({\bf X})^2 > 0$ which implies $\textrm{det}({\bf X}) \neq 0$. A matrix has a non-zero determinant if it is *full column rank* or in other words, if its columns are linearly independent! Therefore, for a unique solution to exist there should be no *multi-collinearity* in the design matrix ${\bf X}$.

**Hat matrix.** Let's have a look at the predictions, which are sometimes called the *fitted values*. Given the optimal linear regression coefficients our model gives predictions based on: 

$$
\begin{equation}
\hat{\vec{y}} = {\bf X}\, \hat{\vec{\beta}} = {\bf X}\, ({\bf X}^{T} {\bf X})^{-1}\, {\bf X}^{T} \vec{y},
\end{equation}
$$

where we simply plugged the solution \eqref{sol} to reach at the second equality. Notice that in the equation above the operator that acts on the target variable $\vec{y}$, outputs the corresponding predictions $\hat{\vec{y}}$. As the preceding statement suggests, we can therefore define the $n \times n$ "hat matrix" as follows: 

$$
\begin{equation}
{\bf H} =  {\bf X}\, ({\bf X}^{T}\, {\bf X})^{-1}\, {\bf X}^{T}. 
\end{equation}
$$
Hat matrix has interesting properties that allow us to categorize it as an orthogonal projection operator. Its orthogonality comes from the fact that ${\bf H}^T = {\bf H}$ while its projective property operator is implied by ${\bf H}^2 = {\bf H}$: e.g. when a test vector is projected onto a subspace, second projection has no effect on the latter. We prove these relations in [Appendix A](#appendix-a). Using the hat matrix ${\bf H}$, we can also define a *residual maker* matrix ${\bf M}$: 

$$
\vec{\mathrm{e}} = \vec{y} - \hat{\vec{y}} = ({\bf I} - {\bf H})\, \vec{y} \equiv {\bf M}\, \vec{y}.
$$

The residual maker matrix is also an orthogonal projection matrix (see [Appendix A](#appendix-a)). Finally, it is not hard to see that ${\bf M}$ and ${\bf H}$ are orthogonal: ${\bf H}{\bf M} = {\bf H}({\bf I} - {\bf H}) = {\bf H} - {\bf H} = 0$. 

Now considering the normal equations, we gain more insight on OLS regressor:

$$
\begin{equation}
{\bf X}^{T} \left(\vec{y} - {\bf X} \hat{\vec{\beta}}\right) = {\bf X}^{T} \vec{\mathrm{e}} = 0.
\end{equation}
$$

This implies that residual vector is orthogonal to the space spanned by the columns of the design matrix ${\bf X}$. In other words, residuals are uncorrelated with the explanatory variables (features)! The projection implied by the hat matrix, i.e. $\hat{\vec{y}} = {\bf H} \vec{y}$, has also a nice geometric interpretation. To understand this we can think of ${\bf H}$ that consist of 3 parts. First, ${\bf X}^T y$ gives us a $p \\times 1$ "overlap"
vector that measures the correlation between $y$ and the columns of the design matrix. Then the $p\times p$ operator $({\bf X}^T {\bf X})^{-1}$ scales the resulting vector taking into account the correlation (${\bf X}^T {\bf X}$) between the columns of ${\bf X}$. The scaled $p \times 1$ vector then is mapped back to the space ($\in \mathbb{R}^n$) spanned by the columns of ${\bf X}$, via the final operator ${\bf X}$. In other words, ${\bf H}$ outputs a weighted combination of the column of ${\bf X}$ to form $\hat{\vec{y}}$. And it does so in a way to preserve information as it is a $\mathbb{R}^n \to \mathbb{R}^n$ map. These arguments make sense as the linear model $\vec{y} = {\bf X} \beta$ is constrained to leave in the space of linear combinations of the columns of the design matrix. Furthermore, since this projection is orthogonal, it does throw away any information that is perpendicular to the space spanned by the columns of ${\bf X}$. In this sense, this projection is closest to the response $\vec{y}$ in Euclidean distance. By these arguments we can also conclude that residual vector lives in the space complementary to the column subspace of the design matrix. This explains intuitively how the OLS regressor do not leave any useful information behind. 

#### Adding the intercept 
-----

So far we focused on linear regression without an intercept. However, adding it can substantially effect the accuracy of the fit. As we mentioned before we can incorporate it by adding a column vector to the predictors that has unit entries: 

$$
\begin{equation}
\vec{y} = \begin{bmatrix}
\bf 1\,\, {\bf X}_2 
\end{bmatrix}\begin{bmatrix}
\beta_1 \\ \vec{\beta}_2
\end{bmatrix} + \epsilon,
\end{equation}
$$

where we stacked the designed matrix horizontally with a $n \times 1$ column vector ${\bf 1} = [1,1,\dots,1]^T$ and ${\bf X}_2$ while stacking the coefficients vertically with a scalar $\beta_1$ and a $p$ vector $\vec{\beta}_2$. The normal equations \eqref{lfd} then can be partitioned as 

$$
\begin{align}
\nonumber {\bf 1}^T\,{\bf 1} \beta_1 + {\bf 1}^T\,{\bf X}_2 \vec{\beta}_2 &= {\bf 1}^T\, \vec{y},\\
{\bf X}_2^T\,{\bf 1} \beta_1 + {\bf X}_2^T\,{\bf X}_2 \vec{\beta}_2 &= {\bf X}_2^T\, \vec{y} \label{neqi}
\end{align}
$$

We can then solve for $\beta_1$ and $\vec{\beta}_2$. We start with $\beta_1$, using the first equation above it reads as

$$
\begin{equation}\label{b1}
\hat{\beta}_1 = \left({\bf 1}^T\,{\bf 1}\right)^{-1}\, {\bf 1}^T \left(\vec{y} - {\bf X}_2 \hat{\vec{\beta}}_2\right) =  \frac{ {\bf 1}^T}{n} \left(\vec{y} - {\bf X}_2 \hat{\vec{\beta}}_2\right),
\end{equation}
$$

noting $\left({\bf 1}^T\,{\bf 1}\right)^{-1} = n^{-1}$. Plugging this equation into the second equation in \eqref{neqi} then gives 

$$
\begin{equation}\label{ib2}
\hat{\vec{\beta}}_2 = \left({\bf X}_2^T\,{\bf M}_1\, {\bf X}_2\right)^{-1}\, {\bf X}_2^T\, {\bf M}_1 \vec{y},
\end{equation}
$$

where we defined hat matrix and residual maker matrix for ${\bf 1}$ as

$$
\begin{align}
\nonumber {\bf H}_1 = {\bf 1}\,  \left({\bf 1}^T\,{\bf 1}\right)^{-1}\, {\bf 1}^T,\\
{\bf M}_1 = {\bf I} - {\bf H}_1.
\end{align}
$$

Notice that the form of the solution for $\hat{\vec{\beta}}_2$ \eqref{ib2} resembles the solution without an intercept, if we were to replace $\vec{y} \to {\bf M}_1 \vec{y}$ and ${\bf X}_2 \to {\bf M}_1\, {\bf X}_2$ by recalling the projective property ${\bf M}_1^2 = {\bf M}_1$. In some sense, \eqref{ib2} imply that we can obtain the optimal regression coefficients by regressing ${\bf M}_1 \vec{y}$ onto ${\bf M}_1\, {\bf X}_2$.

Next, we note that the hat matrix associated with ${\bf 1}$ is a $n\times n$ matrix with all entries equal to $1/n$: 

$$
\begin{equation}
{\bf H}_1 = {\bf 1}\,  \left({\bf 1}^T\,{\bf 1}\right)^{-1}\, {\bf 1}^T = \frac{1}{n} {\bf 1}\, {\bf 1}^T
= \begin{bmatrix} 
    1/n & 1/n & \dots  & 1/n \\
    1/n & 1/n & \dots & 1/n \\
    \vdots & & \ddots & \vdots\\
    1/n & \dots  & \dots & 1/n 
    \end{bmatrix}  
\end{equation}
$$

Therefore, when it acts on a column vector it outputs the average of the components of the vector as the entries of the resulting column vector. This implies that ${\bf M}_1\, \vec{y} = \vec{y} - \bar{\vec{y}}$. Similarly, its action on matrix result with a matrix where each column is a vector with averages. This is useful for simplifying the first term in \eqref{ib2} as 

$$
\begin{align}
{\bf X}_2^T\,{\bf M}_1\, {\bf X}_2 = {\bf X}_2^T\,({\bf I} - {\bf H}_1)\, {\bf X}_2 = {\bf X}_2^T\,({\bf X}_2 - \bar{\bf X}_2),
\end{align}
$$

where $\bar{\bf X}_2$ is a matrix where each column is an $n$-vector with the mean of that respective column in ${\bf X}_2$ repeated $n$ times. With these considerations in mind, we can re-write \eqref{ib2} as

$$
\begin{equation}
\hat{\vec{\beta}}_2 = \left({\bf X}_2^T\,({\bf X}_2 - \bar{\bf X}_2)\right)^{-1}\, {\bf X}_2^T\, (\vec{y} - \bar{\vec{y}})
\end{equation}
$$

Therefore, for an OLS regression with intercept, the optimal coefficients $\hat{\beta}_2$ associated with our predictors in the design matrix are just the result of the normal equations after mean-centering our targets and predictors. This intuitively means that the "slope" (which is now a hyperplane) defined by these coefficients will pass through the origin. It is the scalar $\hat{\beta}_1$ that provides an up-lift/down-lift to this hyperplane. From \eqref{b1}, we obtain this bias as 

$$
\begin{equation}
\hat{\beta}_1 =  \frac{ {\bf 1}^T}{n} \left(\vec{y} - {\bf X}_2 \hat{\vec{\beta}}_2\right) = \bar{y} - \bar{\bf x}_2 \hat{\vec{\beta}}_2,
\end{equation}
$$

where $\bar{\bf x}_2$ is a $1 \times p$ row vector that contains means for each predictor and $\bar{y}$ is scalar of the average of all responses.

#### **References** 
-----------

**1. Topics in Mathematics with Applications in Finance, [Lecture](https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/resources/lecture-6-regression-analysis/) on Regression Analysis, by Dr. Peter Kempthorne, MIT, Sloan School of Management, 2013.**


#### Appendix A: Hat matrix and residual maker as orthogonal projection operators {#appendix-a}

We first prove orthogonality. For the hat matrix, this is simple 

$$
\begin{align}
\nonumber {\bf H}^T &= ({\bf X}\, ({\bf X}^{T}\, {\bf X})^{-1} {\bf X}^{T})^{T},\\
\nonumber &= {\bf X}\, ({\bf X}\, ({\bf X}^{T}\, {\bf X})^{-1})^T,\\
\nonumber &= {\bf X}\, (({\bf X}^{T}\, {\bf X})^{-1} {\bf X}^{T}),\\
&= {\bf H},
\end{align}
$$

where we utilized $(A B)^{T} = B^T A^T$ for matrices and the fact that inversion commutes with transpose operation $(A A^{-1})^T = 1^T = (A^{T})^{-1} A^T$. The projective property directly follows associative property of matrix multiplication: 
$$
\begin{align}
\nonumber {\bf H}^2 &=  {\bf X}\, ({\bf X}^{T}\, {\bf X})^{-1} {\bf X}^{T} \, {\bf X}\, ({\bf X}^{T}\, {\bf X})^{-1} {\bf X}^{T},\\
\nonumber &= {\bf X}\, ({\bf X}^{T}\, {\bf X})^{-1}\, [({\bf X}^{T} \, {\bf X})\,({\bf X}^{T}\, {\bf X})^{-1}]\, {\bf X}^{T},\\
&= {\bf X}\, ({\bf X}^{T}\, {\bf X})^{-1} {\bf X}^{T}.
\end{align}
$$

Given the orthogonal projection matrix ${\bf H}$, it is easier to prove that the residual maker is also an orthogonal projection matrix: 

$$
\begin{align}
\nonumber {\bf M}^{2} &= ({\bf I} - {\bf H}) ({\bf I} - {\bf H}),\\
\nonumber &= {\bf I} - {\bf H} - {\bf H} + {\bf H}^2,\\
\nonumber &= {\bf I} - {\bf H},\\
&= {\bf M}.
\end{align}
$$

Orthogonality is also directly follows from the linearity property of the transpose together with the orthogonality of the $n \times n$ identity matrix ${\bf I}$ and ${\bf H}$: 

$$
{\bf M}^{T} = ({\bf I} -{\bf H})^{T} = ({\bf I}^T - {\bf H}^T) = ({\bf I} - {\bf H}) = {\bf M}.
$$
